\listfiles
\documentclass[twoside,12pt]{article}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage[top=2in, bottom=1.5in, left=0.85in, right=0.5in]{geometry}
\usepackage[hyphenbreaks]{breakurl}
%\usepackage[pdfstartview=FitH,pdfstartpage=13,pdfpagemode=UseNone]{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx} 
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage{mdwlist }
\usepackage{color}
\usepackage{multirow}
\usepackage{listings}
\usepackage{float}
\usepackage{setspace}
\usepackage[english]{babel}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}


\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\newtheorem{Dfn}{Definition}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\newcommand{\sign}{\text{sign}}
\newcommand{\argmin}{\arg\!\max}
\begin{document}

\title{Parameter estimation for text analysis\\  Learning Algorithms, Project 3}
\author{Mohsen Malmir, Erfan Sayyari}
\maketitle
\section{Abstract}


\section{Introduction}



\section{Design and Analysis of Algorithm}

\subsection{The bag-of-words representation}
A document is a collection of words and the structure that puts these words together. If we only look at words, we find some words that are not correlated with any topic. These are words that can be found in any document with any topic. Examples of these words are pronouns (you, he, it) connectives (and, because, however), prepositions (to, of, before), auxiliaries (have, been, can), and generic nouns (amount, part, nothing). For the purpose of topic selection, we discard these words from documents as clearly they don't help in picking the right topic. Then we take the union of all words in corpus and call it \emph{dictionary}.

In order to determine a topic for a document, we have to represent  documents using a set of features. We adapt a representation called \emph{bag of words}, in which each document is represented as a histogram over the dictionary. Let $V$ be the total number of words in the dictionary, then each document is represented as a $V$-dimensional vector $x$, where $x[i]$ is the number of times the $i$th word appeared in the document. We can calculate the length of the document as $n=\sum_{m}x_j.$

\subsection{Latent Dirichlet Allocation Model}
To learn the topic distribution for documents, we use the model shown in figure \ref{figGM}. This is a model of $M$ documents, each with length $n_m$. Each word in a document comes from a \emph{topic} $z$. A topic is modeled as a multinomial distribution over the dictionary, with $\phi_k$ as vector. Note that $\phi_k[i]$ denotes the probability that topic $k$ gives to the $i$th word in the dictionary. A document is modeled as a multinomial distribution over topics, with $\theta$ representing the parameters. Again, $\theta_m[i]$ represents the probability of having topic $i$ in the $m$th document. Finally, we put Dirichlet priors over document and topic distributions. The prior over documents is represented by the node $\alpha$, and denotes the prior belief in the distribution of topics in different documents. Also, the prior over topics is represented by node $\beta$, and denotes the prior belief of distribution of words in topics.

\begin{figure}[h!]
\centering
\includegraphics[width=.3\textwidth]{./figs/gm.pdf}
\caption{Plate notation of the LDA graphical model. The shaded node $w$ is observed. }
\label{figGM}
\end{figure}

The probability distribution represented by the graphical model in figure \ref{figGM} is,

\begin{align}
P(\alpha,\Theta,&Z,\beta,\Phi,W)  \\  &=P(\alpha) P(\beta) P(\Theta | \alpha) P(\Phi|\beta) P(Z|\Theta) P(W|Z,\Phi) \\ &= Dir(\alpha) Dir(\beta) \prod_{m=1}^M \text{Multi}(\theta_m | \alpha) \prod_{k=1}^K \text{Multi}(\phi_k | \beta) \prod_{t=1}^T \text{Multi}(w_t | \phi_{z_t})
\end{align}
 
 In the formula above, $\Phi = \{\phi_k, k=1,\ldots,K\}$ and $\Theta=\{\theta_m, m=1,\ldots,M\}$, $Dir$ represents Dirichlet and Multi represents the multinomial distributions. Also, $W = \{w_t, t=1,\ldots,T\}$ represents the entire set of words in the corpus. Note that the prior over topics and documents is chosen to be conjugate,
 
\begin{align}
\gamma \sim Dir(\alpha) \Rightarrow P(\gamma | \alpha) = \frac{\Gamma(\sum_{i=1}^M \alpha_i)}{\prod_{i=1}^M \Gamma(\alpha_i)} \prod_{i=1}^M \gamma_i^{\alpha_i -1} 
\end{align}
where
\begin{align}
\Gamma(t) = \int_0^\infty x^{t-1} e^{-x} dx
\end{align}
is the extension of Factorial function to continuous variables.
\subsection{ LDA Generative process}
To better understand the LDA model, we can look at how documents are generated in this model. First, $\phi_k, k=1,\ldots,K$ are randomly drawn from $Dir(\beta)$. Then For document $m, m=1,\ldots,M$, $\theta_m$ is sampled from $Dir(\alpha)$. Then $n_m$ topics are randomly drawn from Multi$(\theta_m)$. Given $z_t$, the $t$th word is selected from $\phi_{z_t}$. Remember that Gibbs sampling works by sampling from the posterior of variables, one variable at a time. 


Inference includes determining $\theta_m, \phi_k$ for $m=1,\ldots,M,\;\; k=1,\ldots,K$. Note that this is an unsupervised learning problem, where the training vectors are not accompanied by a target value. We propose a model, shown in figure \ref{figGM}, and we want to adapt the model parameters to the training data. A major drawback of the LDA topic model is that we cannot calculate the likelihood of the data analytically. In such situations, MCMC methods come to our help. Particularly, we use Gibbs sampling to infer the model parameters $\theta_m$ and $\phi_k$. First, note that we can integrate out $\theta$ and $\phi$ nodes of the LDA model. This is due to the fact that the distribution of $\alpha$ and $\theta$ are conjugate. Also, this is the case for $\beta$ and $\phi$. Since $w$ is observed, Gibbs sampling reduces to sampling $z_t$ nodes in the model. Given the current values for all other $z$ variables, which we call $Z_{-t}$, and all words $W$, we have
\begin{align}
p(z_t|Z_{-t},W)=\frac{p(Z,W)}{P(Z_{-t},W)}=\frac{p(W|Z)p(Z)}{p(w_t|Z_{-t})p(W_{-t}|Z_{-t})p(Z_{-t})}
\end{align}
In the lectures, it was shown that,
\begin{equation}
p(z_t=j|Z_{-t},W)\propto \frac{q'_{jw_t}+\beta_{w_t}}{\sum_{t'} q'_{jw_{t'}}+\beta_{t'}}\frac{n'_{mj}+\alpha_j}{\sum_k n'_{mk}+\alpha_k}.
\end{equation}
where $m$ is the document to which $w_t$ belongs, $q'_{jw_{x}}$ is the number of times word $w_x$ appeared in topic $j$ and $n'_{mj}$ is the number of times topic $j$ was used in document $m$. Also $\alpha_x$ and $\beta_x$ refer the the $x$th element of vectors $\alpha$ and $\beta$ respectively.

In each epoch, we use all of the words in corpus to update topic of that word, and to update probability of $p(z_i=j|\overline{z'},\bar{w})$. So, for each position, Equation 10 is calculated for different topics. Evaluating this probability for each topic (say $j$) requires constant time. Therefor, time complexity of each epoch of Gibbs sampling is $O(NK)$, where $N$ is the total number of words in corpus and $K$ is the number of topics.

\subsection{Multinomial parameters}
The final step is to find multinomial parameter sets $\underline{\Theta}$ and $\underline{\Phi}$. According to their meaning we could say that:
\begin{equation}
p(\overline{\theta_m}|\bar{w},\bar{z},\alpha)=Dir(\overline{\theta_m}|\overline{n}_m+\bar{alpha})
\end{equation}
\begin{equation}
p(equation{\phi_k}|\bar{w},\bar{z},\alpha)=Dir(\overline{\phi_k}|\overline{q}_k+\bar{beta})
\end{equation}
where $\overline{n_m}$ is the vector of topic observation counts for document m and $\overline{n_k}$ is the vector of term observation counts for topic k. So we will have:
\begin{equation}
\phi_{k,t}=\frac{q_k^{(t)}+\beta_t}{\sum_{t=1}^V q_k^{t}+\beta_t},
\end{equation}
\begin{equation}
\theta_{m,k}=\frac{n_m^{(k)}+\alpha_k}{\sum_{k=1}^K n_m^{k}+\alpha_k}.
\end{equation}

\section{Design of Experiments}

\subsection{Dataset Selection}
The experiments on the model described above was done using two datasets: classic400 and dailyKOS. Table \ref{tableDatasetStats} displays the number of documents, words and total number of words in these datasets. Classic400 is a small dataset, and the fact that it is accompanied by true labels makes it appealing for experiments. DailyKOS on the other hand is a larger dataset, and though it doesn't have the true labels, its large number of documents and total number of words makes it a good test to the performance of the proposed model. 


\begin{table}
\vspace{-2cm}
\center
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Classic400} & \textbf{DailyKOS} \\
 \hline
\textbf{ number of Documents} & 400 & 3430 \\
\textbf{ Dictionary Size} & 6205 & 6906 \\
\textbf{ Total Number of Words} & 31516 & 467714\\
 \hline
\end{tabular}
\caption{Statistics of classic400 and DailyKOS datasets.}
\label{tableDatasetStats}
\end{table}

\subsection{Analyzing topic models}
\subsubsection{Perplexity}
A very general criterion to analyze overfitting of a clustering model is to use perplexity (????). The reciprocal geometric mean of the likelihood of testing data given the trained model $M={\{\Theta,\Phi\}}$ is defined as perplexity. The lower perplexity value shows that the model could fit the testing data better. Mathematically we could define it as:
\begin{equation}
p(\overline{w}|M)=\prod_{m=1}^M p(w_m|M)^{\frac{-1}{N}}=exp - \frac{\sum_{m=1}^Ml og p(w_m|M)}{\sum_{m=1}^M N_m}
\end{equation}
and $log p(w_m|M)$ could be derived from the multinomial parameters:
\begin{equation}
log p(w_m|M)=\prod_{n=1}^{N_m}(\sum_{k=1}^{K} \phi_{k,t}\theta_{m,k})^{q_m^t}
\end{equation}
In these equations, $M$ is number of documents, $N$ is number of all words, $N_m$ is number of words in document $m$, $w_m$ is words in document $m$,$\phi_{k,t}$ is the word distribution for topic $k$ and word $t$, and $\theta_{m,k}$ is the topic distribution for document $m$ and topic $k$.

\subsubsection{Harmonic Mean Method}
In order to investigate how well the trained model can fit to the training data, we use goodness-of-fit. (?????) Harmonic mean for LDA model in (ref????) is defined as:
\begin{equation}
\frac{1}{M}\sum_{m=1}^{M}(\frac{1}{K}\sum_{k=1}^{K}\theta_{m,k})^{-1}
\end{equation}
where $\theta_{m,k}$ is the topic distribution for document m and topic k. If the topic distribution has higher probability for only some topics, its associated harmonic mean value would be lower. So, the model could have better ability to separate documents into different topics.


\subsection{Hyperparameters Selection}
There are two sets of hyper parameters in our model: $\alpha$ which is the parameters of the Dircihlet prior over documents and $\beta$ which is the vector of parameters for Dirichlet prior over topics. One way to select these parameters is to use expert knowledge in determining these parameters. In this case, the expert can give an estimate of the distribution of topics inside documents or distribution of words for different topics. Despite enriching the model, this method seems infeasible for us since we have no source of expertise in this area. Another way is to use an uninformative uniform prior, which acts as psuedo-counts to prevent technical difficulties.\\
Here, we adapt the same parameter values as \cite{fastlda}: $\alpha \in \{0.01,0.1\}$ and $\beta\in\{2/k,0.2/k\}$. These values make sense since they prefer sparse distributions for $\theta$ and $\phi$. Usually documents contain a few topics, therefore a document is usually a sparse distribution over topics. Also when the dictionary size is large, it makes sense to have a topic that is sparse, i.e. it gives higher probability to a few words. Beside the sparsity, there is no other information we are incorporating in our priors. 

\section{Results}


\subsection{Classic400}
We train 4 models on classic400 dataset using prior values mentioned before. Table \ref{tableClssicRecognition} shows the recognition rate for predicting the correct topic for documents in this dataset. For each document, we pick the topic with maximum probability as predicted topic for that document. Then all mappings from the predicted topics to the real topics are investigated. The results shown in table \ref{tableClassicRecognition} represent the best results found on this mapping. These rates indicate that the proposed model fits the data very well.

One should also note the effect of hyper parameters on recognition rates. Among four different values, the model with the lowest recognition rate is the for which the prior parameters enforce the least amount of sparsity of documents and topics. In our experiments, we tried less sparse values, eg. $\beta=10$, but the recognition rates were not good and the resulting topic distributions were a mix of different topics with no observable preference for a specific topic.

Measuring the goodness-of-fit for unsupervised models is a difficult task, however the structure of problem at hand can be exploited to investigate if the model has learned any meaningful pattern in the input data. For the present problem, we can observe if the set of words inside learned topics are semantically similar. Table \ref{tableTopWordsClassic} displays the top 20 words for each topic. It is interesting to look at this table, as the words in each row totally make sense. For example, the words in the third row represent the medical topic for documents.

Figure \ref{figClassicDist} shows the distribution of documents based on $\theta_m$. In this figure, document $m$ is represented in 3D space with the normalized count vector $\theta_m$ obtained from Gibbs sampling. The best recognition rate is obtained from the model shown in left side of bottom row in figure \ref{figClassicDist}. One thing to note here is that this is the most sparse representation of documents among all 4 models. For all figures here, the documents are mainly concentrated at vertices of the 2D simplex. 


\begin{table}[!]
\begin{center}
\begin{tabular}{| c | c |} 
\hline
\textbf{Hyper paramteres}& \textbf{Topic prediction rate}  \\ \hline

$\alpha=0.01,\beta=2.0$ & $0.9725$ \\ \hline
$\alpha=0.1,\beta=2.0$ &  $0.9375$ \\ \hline
$\alpha=0.01,\beta=0.2$ & $0.97$ \\ \hline
$\alpha=0.1,\beta=0.2$ & $0.96$ \\ \hline
 
\end{tabular}
\caption{Recognition rate for predicting the correct topic for each document for classic400 dataset.}
\label{tableClssicRecognition}
\end{center}
\end{table}

\begin{table}[!]
\begin{center}
\begin{tabular}{| c | p{12cm} |}
\hline
\textbf{Topic}& \textbf{Top 20 Words}  \\ \hline
\textbf{1}&boundary,layer,wing,mach,supersonic,wings,ratio,velocity,schlock, effects,surface,plate,solution,jet,lift,numbers,high,edges,speeds,head\\ \hline
\textbf{2}&system,scientific,retrieval,research,language,science,journals, methods,systems,subject,library,request,classification,data,problems, field,development,considered,general,structure\\
 \hline
\textbf{3}&ventricular,fatty,left,nickel,cases,acids,aortic,blood,normal,glucose, spetal,acid,pulmonary,defect, visual,time,clinical,free,ffa\\
 \hline
 
\end{tabular}
\caption{}
\label{tableTopWordsClassic}
\end{center}
\end{table}




\begin{figure}
        \centering
        \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\textwidth]{figs/classicalpha01beta2.png}
                \caption{$\alpha=0.1,\;\beta=2.$}
                \label{fig:gull}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\textwidth]{figs/classicalpha01beta02.png}
                \caption{$\alpha=0.1,\;\beta=0.2$}
                \label{fig:tiger}
        \end{subfigure}
        
        \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\textwidth]{figs/classicalpha001beta2.png}
                \caption{$\alpha=0.01,\;\beta=2.$}
                \label{fig:mouse}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
                  \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\textwidth]{figs/classicalpha001beta02.png}
                \caption{$\alpha=0.01,\;\beta=0.2$}
                \label{fig:mouse}
        \end{subfigure}
        \caption{Distribution of documents after training on classic400 dataset. Each document is represented by its distribution over 3 topics.}
        \label{figClassicDist}
\end{figure}


\subsection{DailyKOS}

\section{Discussion}


\begin{thebibliography}{9}
\bibitem{fastlda}
Porteous, Ian, et al. "Fast collapsed gibbs sampling for latent dirichlet allocation." Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2008.

\end{thebibliography}




\end{document}
