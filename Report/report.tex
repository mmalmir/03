\listfiles
\documentclass[twoside,12pt]{article}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage[top=2in, bottom=1.5in, left=0.85in, right=0.5in]{geometry}
\usepackage[hyphenbreaks]{breakurl}
%\usepackage[pdfstartview=FitH,pdfstartpage=13,pdfpagemode=UseNone]{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx} 
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage{mdwlist }
\usepackage{color}
\usepackage{multirow}
\usepackage{listings}
\usepackage{float}
\usepackage{setspace}
\usepackage[english]{babel}

\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\newtheorem{Dfn}{Definition}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\newcommand{\sign}{\text{sign}}
\newcommand{\argmin}{\arg\!\max}
\begin{document}

\title{Parameter estimation for text analysis\\  Learning Algorithms, Project 3}
\author{Mohsen Malmir, Erfan Sayyari}
\maketitle
\section{Abstract}


\section{Introduction}



\section{Design and Analysis of Algorithm}

\subsection{The bag-of-words representation}
Vocabulary is the collection of words that are used in documents at least once. In text mining, many eliminated from the vocabulary "stop" words. These are some common words in most documents and they are not related to a particular subject or topic. From linguistic point of view, these words are called "function words" as well. They include pronouns (you, he, it) connectives (and, because, however), prepositions (to, of, before), auxiliaries (have, been, can), and generic nouns (amount, part, nothing). 

After constructing our vocabulary, we need to find a way to represent our documents. Each document is represented as a vector of length V (length of vocabulary), where each elements of this vector describes number of appearance of a particular word in the document. If this vector is $x$ then its $j^{th}$ element, $x_j$, is the number of appearance of word j from vocabulary in the document. The length of document is $n=\sum_{m}x_j.$
\subsection{The multinomial distribution}
After representing documents, we need to select a model for a set of documents. This model is a probability model, and we want to use maximum likelihood to find the best parameters that best represent our training data. The probability distribution that we use is the multinomial distribution. It could be represented mathematically as:

\begin{equation}
p(x;\theta)=(\frac{n!}{\prod_{j=1}^{m}x_j!})(\prod_{j=1}^{m}\theta_j^{x_j}).
\end{equation}
where data x is a vector of non-negative integers and the parameters $\theta$ are real-valued vectors. Moreover, the sum of elements of vector $\theta$ have to be equal to 1: $\sum_{j=1}^{m}\theta_j=1$.

Computing the probability of a document requires $O(n)$, where n is the number of words in a document. It is due to the fact that in a single document, most of words in vocabulary do not contribute and $\theta_j^{x_j}=1$ and $x_j!=1$.

The log probability of the probability of a document is equal to:
\begin{equation}
\log p(x;\theta)=\log n!-[\sum_{j=1}^m\log x_j!]+[\sum_{j=1}^m x_j.\log \theta_j]
\end{equation}
Given a set of training documents, the maximum likelihood estimate of the $j^th$ $\theta$ (parameter) is:
\begin{equation}
\theta_j = \frac{1}{T}\sum_x x_j
\end{equation}
where the sum is over the entire documents $x$ in training set. $T$ in above equation is a normalization factor and it is equal to: $T=\sum_x \sum_j x_j$, which is the total number of words in all documents of training set. 

In order to avoid probabilities that are perfectly zero, we set:
\begin{equation}
\theta_j=\frac{1}{T'}(c+\sum_x x_j).
\end{equation}
The constant $c$ is called a pseudo-count, and it relates to our prior knowledge of our parameters.
 
\subsection{ Generative processes}
In order to do unsupervised learning over a set of training documents, a common way is to assume that data were generated by some random process, and then we want to infer the parameters of the process. In generative process, we use maximum likelihood or some revised versions of it such as maximum a posteriori (MAP) probability for learning. A generative process for a single document is as follows: \\
A: fix a multinomial distribution with parameter vector $\phi$ of length $V$. \\
B: for each word in the document \\
draw a word $w$ according to $\phi$ 

For a collection of documents of multiple topics, a generative process is:
A: fix a multinomial $\alpha$ over categories 1 to K for category number 1 to category number K \\
for category number 1 to category number K \\
fix a multinomial with parameter vector $\phi_k$
B: for document number 1 to document number M
draw a category $z$ according to $\alpha$
for each word in the document
draw a word $w$ according to $\phi_z$

$z$ is between 1 and K, and for each document, the value of $z$ is hidden. So, the global probability distribution is equal to:
\begin{equation}
f(x)=\sum_{k=1}^K \alpha_k f(x;\alpha_k).
\end{equation}
where x is a document and $\phi_k$ is the parameter vector of the $k^{th}$ multinomial. K is called the number of components in the mixture model. 

\subsection{Latent Dirichlet allocation}
The model that we use is called Latent Dirichlet Allocation (LDA). The choice of LDA is based on the intuition that each document contains multiple topics. The Dirichlet is a valid prior for the multinomial distribution. Concretely, it is a probability is a probability density function over the set of all multinomial parameter vectors. The Dirichlet distribution has parameters which are represented as vector $\alpha$ of length $m$. generative process assumed by the LDA model is as follows:

Given: Dirichlet distribution with parameter vector $\alpha$ of length $K$ \\ 
Given: Dirichlet distribution with parameter vector $\beta$ of length $V$ \\
for topic number 1 to topic number $K$ \\ 
draw a multinomial with parameter vector $\phi_k$ according to $\beta$ \\
for document number 1 to document number $M$\\
draw a topic distribution, a multinomial $\theta$ according to $\alpha$ \\
for each word in the document \\
draw a topic $z$ according to $\theta$ \\
draw a word $w$ according to $\phi_z$ \\
Note that $z$ is an integer between 1 and $K$ for each word. 

The equation for the Dirichlet distribution is:
\begin{equation}
p(\gamma|\alpha)=\frac{1}{D(\alpha)}\prod_{s=1}^{m}\gamma_s^{\alpha_s-1}.
\end{equation}


D is a normalization constant which is equal to:
\begin{equation}
D(\alpha)=\int_{\gamma}\prod_{s=1}^m \gamma_s^{\alpha_s-1}
\end{equation}
or alternatively it is equal to:
\begin{equation}
D(\alpha)=\frac{\prod_{s=1}^m \gamma(\alpha_s)}{\gamma(\sum_{s=1}^{m}\alpha_s)}
\end{equation}
where $\gamma$ is the standard continuous generalization of the factorial function that, $\gamma(k)=(k-1)!$ for integer $k$.
\subsection{Training via Gibbs sampling}
The training data are the words in all documents, and the prior distribution parameters $\alpha$, $\beta$, the number of topics $K$, number of documents $M$, length of each document $N_m$, and the length of vocabulary $V$ are assumed to be fixed and known.

Moreover, we suppose that we know the $z$ value for every word except for word number $i$. We want to draw the value for $z_i$ based on the probability distribution of topics and words. We use the notation $\overline{w'}$ to mean $\bar{w}$ with word number $i$ removed, so $\bar{w}=\{w_i,\overline{w'}\}$. Similarly, we write $\bar{z}=\{z_i,\overline{z'}\}$, and we need to compute the probability of $z_i$ given $\overline{z'},\bar{w}$:
\begin{equation}
p(z_i|\overline{z'},\bar{w})=\frac{p(\bar{z},\bar{w})}{\overline{z'},\bar{w}}=\frac{p(\bar{w}|\bar{z})p(\bar{z})}{p(w_i|\overline{z'})p(\overline{w'}|\overline{z'})p(\overline{z'})}
\end{equation}
for $z_i=1$ to $z_i=K$. After simplifications we could write this probability as:
\begin{equation}
p(z_i=j|\overline{z'},\bar{w})\propto \frac{q'_{jw_j}+\beta_{w_j}}{\sum_t q'_{jt}+\beta_t}\frac{n'_{mj}+\alpha_j}{\sum_k n'_{mk}+\alpha_k}.
\end{equation}
where $q_{kt}$ is the number of times that word $t$ occurs with topic $k$ in the whole corpus, and $q'_{jt}$ is equal to $q_{jt}$ with one subtracted from the count for word $t$ with topic $j$. $\overline{n'}_{m}$ is $\overline{n}_m$ with one subtracted from the count for topic $z_i$, and $\overline{n}_m$ is the vector of topic counts $\langle n_{m1},\ldots,n_{mK} \rangle$ for document number $m$.

\section{Design of Experiments}

\subsection{Dataset Selection}
The experiments on the model described above was done using two datasets: classic400 and dailyKOS. Table \ref{tableDatasetStats} display the number of documents, words and total number of words in these datasets. Classic400 is a small dataset, and the fact that it is accompanied by true labels makes it appealing for experiments. DailyKOS on the other hand is a larger dataset, and though it doesn't have the true labels, its large number of documents and total number of words makes it a good test to the performance of the proposed model. 


\begin{table}
\vspace{-2cm}
\center
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Classic400} & \textbf{DailyKOS} \\
 \hline
\textbf{ number of Documents} & 400 & 3430 \\
\textbf{ Dictionary Size} & 6205 & 6906 \\
\textbf{ Total Number of Words} & 31516 & 467714\\
 \hline
\end{tabular}
\caption{Statistics of classic400 and DailyKOS datasets.}
\label{tableDatasetStats}
\end{table}


\subsection{Hyperparameters Selection}
There are two sets of hyper parameters in our model: $\alpha$ which is the parameters of the Dircihlet prior over documents and $\beta$ which is the vector of parameters for Dirichlet prior over topics. One way to select these parameters is to use expert knowledge in determining these parameters. In this case, the expert can give an estimate of the distribution of topics inside documents or distribution of words for different topics. Despite enriching the model, this method seems infeasible for us since we have no source of expertise in this area. Another way is to use an uninformative uniform prior, which acts as psuedo-counts to prevent technical difficulties.\\
Here, we adapt the same parameter values as \cite{fastlda}: $\alpha \in \{0.01,0.1\}$ and $\beta\in\{2/k,0.2/k\}$. These value make sense since they prefer sparse distributions for $\theta$ and $\phi$. Usually documents contain a few topics, therefore a document is usually a sparse distribution over topics. Also when the dictionary size is large, it makes sense to have a topic that is sparse, i.e. it gives higher probability to a few words. Besides the sparsity, there is no other information we are incorporating in our priors. 

\section{Results}
\clearpage
\subsection{Classic400}

\begin{table}[!]
\vspace{-2cm}
\begin{center}
\begin{tabular}{| c | p{12cm} |}
\hline
\textbf{Topic}& \textbf{Words}  \\ \hline
1&boundary,layer,wing,mach,supersonic,wings,ratio,velocity,schlock, effects,surface,plate,solution,jet,lift,numbers,high,edges,speeds,head\\ \hline
2&system,scientific,retrieval,research,language,science,journals, methods,systems,subject,library,request,classification,data,problems, field,development,considered,general,structure\\
 \hline
3&ventricular,fatty,left,nickel,cases,acids,aortic,blood,normal,glucose, spetal,acid,pulmonary,defect, visual,time,clinical,free,ffa\\
 \hline
 
\end{tabular}
\caption{}\label{table:2}
\end{center}
\end{table}


\subsection{DailyKOS}

\section{Discussion}


\begin{thebibliography}{9}
\bibitem{fastlda}
Porteous, Ian, et al. "Fast collapsed gibbs sampling for latent dirichlet allocation." Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2008.

\end{thebibliography}




\end{document}
