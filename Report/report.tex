\listfiles
\documentclass[twoside,12pt]{article}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage[top=2in, bottom=1.5in, left=0.85in, right=0.5in]{geometry}
\usepackage[hyphenbreaks]{breakurl}
%\usepackage[pdfstartview=FitH,pdfstartpage=13,pdfpagemode=UseNone]{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx} 
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage{mdwlist }
\usepackage{color}
\usepackage{multirow}
\usepackage{listings}
\usepackage{float}
\usepackage{setspace}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\newtheorem{Dfn}{Definition}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\newcommand{\sign}{\text{sign}}
\newcommand{\argmin}{\arg\!\max}
\begin{document}

\title{Parameter estimation for text analysis\\  Learning Algorithms, Project 3}
\author{Mohsen Malmir, Erfan Sayyari}
\maketitle
\section{Abstract}


\section{Introduction}



\section{Design and Analysis of Algorithm}

\subsection{The bag-of-words representation}
Vocabulary is the collection of words that are used in documents at least once. In text mining, many eliminated from the vocabulary "stop" words. These are some common words in most documents and they are not related to a particular subject or topic. From linguistic point of view, these words are called "function words" as well. They include pronouns (you, he, it) connectives (and, because, however), prepositions (to, of, before), auxiliaries (have, been, can), and generic nouns (amount, part, nothing). 

After constructing our vocabulary, we need to find a way to represent our documents. Each document is represented as a vector of length V (length of vocabulary), where each elements of this vector describes number of appearance of a particular word in the document. If this vector is $x$ then its $j^{th}$ element, $x_j$, is the number of appearance of word j from vocabulary in the document. The length of document is $n=\sum_{m}x_j.$
\subsection{The multinomial distribution}
After representing documents, we need to select a model for a set of documents. This model is a probability model, and we want to use maximum likelihood to find the best parameters that best represent our training data. The probability distribution that we use is the multinomial distribution. It could be represented mathematically as:

\begin{equation}
p(x;\theta)=(\frac{n!}{\prod_{j=1}^{m}x_j!})(\prod_{j=1}^{m}\theta_j^{x_j}).
\end{equation}
where data x is a vector of non-negative integers and the parameters $\theta$ are real-valued vectors. Moreover, the sum of elements of vector $\theta$ have to be equal to 1: $\sum_{j=1}^{m}\theta_j=1$.

Computing the probability of a document requires $O(n)$, where n is the number of words in a document. It is due to the fact that in a single document, most of words in vocabulary do not contribute and $\theta_j^{x_j}=1$ and $x_j!=1$.

The log probability of the probability of a document is equal to:
\begin{equation}
\log p(x;\theta)=\log n!-[\sum_{j=1}^m\log x_j!]+[\sum_{j=1}^m x_j.\log \theta_j]
\end{equation}
Given a set of training documents, the maximum likelihood estimate of the $j^th$ $\theta$ (parameter) is:
\begin{equation}
\theta_j = \frac{1}{T}\sum_x x_j
\end{equation}
where the sum is over the entire documents $x$ in training set. $T$ in above equation is a normalization factor and it is equal to: $T=\sum_x \sum_j x_j$, which is the total number of words in all documents of training set. 

In order to avoid probabilities that are perfectly zero, we set:
\begin{equation}
\theta_j=\frac{1}{T'}(c+\sum_x x_j).
\end{equation}
The constant $c$ is called a pseudo-count, and it relates to our prior knowledge of our parameters.
 
\subsection{ Generative processes}
In order to do unsupervised learning over a set of training documents, a common way is to assume that data were generated by some random process, and then we want to infer the parameters of the process. In generative process, we use maximum likelihood or some revised versions of it such as maximum a posteriori (MAP) probability for learning. A generative process for a single document is as follows: \\
A: fix a multinomial distribution with parameter vector $\phi$ of length $V$. \\
B: for each word in the document \\draw a word $w$ according to $\phi$
%%%%%%%%%%%%%%%%%%



A single multinomial distribution can only represent a category of closely re- lated documents. For a collection of documents of multiple categories, a simple generative process isA: fix a multinomial ? over categories 1 to K for category number 1 to category number Kfix a multinomial with parameter vector ?k B: for document number 1 to document number Mdraw a category z according to ? for each word in the documentdraw a word w according to ?z
Note that z is an integer between 1 and K. For each document, the value of z is hidden, meaning that it exists conceptually, but it is never known, not even for training data. The generative process above gives the following global probability distribution:Kf(x) = ?? ?kf(x; ?k).k=1

where x is a document and ?k is the parameter vector of the kth multinomial. K is called the number of components in the mixture model. For each k, f(x;?k) is the distribution of component number k. The scalar ?k is the proportion of component number k. A distribution like this is called a mixture distribution. In general, the components can be probability density functions (pdfs), for example Gaussians, or probability mass functions (pmfs), for example multinomials.
\subsection{Latent Dirichlet allocation}

\subsection{Training via Gibbs sampling}

\subsection{ Implementation notes}

\section{Design of Experiments}


\subsection{Feature extraction}

\subsection{Initialization}

\subsection{Performance Measure}


\subsection{Contrastive Divergence}

\subsubsection{Training with Bounded Weights}


\subsection{Model Averaging}


\subsection{Comparison Between Different Models}


\section{Discussion}


\begin{thebibliography}{9}


\end{thebibliography}




\end{document}
